1)

Addresses in a source program are generally symbolic (such as "count").

A compiler will bind these symbolic addresses to "relocatable" addresses,
such as 16 bytes from the beginning of this module.

A linker or loader will bind these relocatable addresses to absolute
addresses (such as 74016).

Each binding is a mapping from one address space to an, often more low-level,
other address space. 

The binding of instructions and data to memory addresses can be done
early or late.

compile-time: we know at compile-time where this process will reside in
memory, then absolute code can be generated, e.g., count -> 0ff2. If the
location where this code will reside in memory changes at a later time,
then the program needs to be re-compiled.

load-time: if we do not know at compile-time where this process will reside
in memory, then the compile cannot generate absolute code, but rather
must generate relocatable code.  In this case, the binding of, e.g.,
count -> 0ff2 is delayed until load-time. If the starting address changes,
then we need only reload the program to incorporate this changed value.

execution-time: if the process can be moved from one memory segment to
another during execution, e.g., paging, then the binding must be delayed
until run-time, e.g., count -> 0ff2 at t_0 and 0ffa at t_1.

2)

first fit:

100k

500k p1
200k p3

300k

600k p2

p4 must wait in the job queue until memory becomes available.

best fit:

100k

500k p2
200k p3

300k p1

600k p4

worst fit:

100k

500k p2
200k 

300k p3

600k p1

p4 must wait in the job queue until memory becomes available.

The best fit algorithm makes the most efficient use of memory.

3)
                                +=============+=============+=========+
                                |  ex. frag.  | int. frag.  |  share  |
+===============================+=============+=============+=========+
| contiguous-memory allocation  |    yes      |    yes or no|  no     |
+=============================================+=============+=========+
| pure paging                   |    no       |    some     |  yes    |
+=============================================+=============+=========+
| pure segmentation             |    yes      | very little |  yes    |
+=====================================================================+


4)

a) Since it is impossible to determine the maximum memory required for the
stack at load time, we must make a generous estimate and, therefore, often
waste memory, resulting in a potential great deal of internal
fragmentation and a lower degree of multiprogramming.

b) As the stack grows and shrinks at run-time, we can create and destroy pages,
respectively and, therefore, make more efficient use of memory, resulting in
very little internal fragmentation and a higher degree of multiprogramming.

c) At compile-time, compiler automatically constructs segments, including the
stack segment, reflecting the input program.  Again, since it is impossible to
determine the maximum memory required for the stack at compile-time, the
compiler must make a generous estimate and, therefore, often waste memory,
resulting in a potential great deal of internal fragmentation and a lower
degree of multiprogramming.

5)

a) 400 ns (200ns to access the page map table and 200ns to access the memory
referenced)

b) 250 ns

hit: 200 ns
miss: 400 ns (see part a)

0.75(200) + 0.25(400) = 150 + 100 = 250ns

6)

a) 0,430 -> 649
b) 1,10 -> 2310
c) segmentation fault, 500 is greater than the length of the segment (100)
d) 3,400 -> 1727
e) segmentation fault, 112 is greater than the length of the segment (96)
